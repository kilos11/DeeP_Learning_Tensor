{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilos11/DeeP_Learning_Tensor/blob/main/Spam_Classification_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c593e910",
      "metadata": {
        "id": "c593e910"
      },
      "source": [
        "### 5.1. Setup\n",
        "\n",
        "Install required text processing libraries for the example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgSuvt-bXOUx",
        "outputId": "2a80e683-469d-4b06-a776-919c76be6b88"
      },
      "id": "CgSuvt-bXOUx",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "91d8262a",
      "metadata": {
        "id": "91d8262a",
        "outputId": "a631d086-de06-4c80-db88-cc96c41a0785",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#!pip install nltk\n",
        "\n",
        "# Import the Natural Language Toolkit (nltk) library\n",
        "import nltk\n",
        "\n",
        "# Download the NLTK stopwords dataset\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download the NLTK punkt dataset for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import the stopwords module from the NLTK corpus\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the NLTK WordNet dataset\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Import the WordNetLemmatizer module from the NLTK stem package\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create an instance of the WordNetLemmatizer for lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40c3a4f",
      "metadata": {
        "id": "f40c3a4f"
      },
      "source": [
        "### 5.2. Creating Text Representations\n",
        "\n",
        "Text data needs to be converted to numeric representations before they can be used to train deep learning models. The Spam classification feature data is converted to TF-IDF vectors and the target variable is converted to one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "08900f4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08900f4b",
        "outputId": "56c65332-f8f1-4f7f-fc62-37b3305c4d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded Data :\n",
            "------------------------------------\n",
            "  CLASS                                                SMS\n",
            "0   ham   said kiss, kiss, i can't do the sound effects...\n",
            "1   ham      &lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\n",
            "2  spam  (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
            "3  spam  * FREE* POLYPHONIC RINGTONE Text SUPER to 8713...\n",
            "4  spam  **FREE MESSAGE**Thanks for using the Auction S...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "#Load Spam Data and review content\n",
        "spam_data = pd.read_csv(\"/content/drive/MyDrive/IRIS/Spam-Classification.csv\")\n",
        "\n",
        "print(\"\\nLoaded Data :\\n------------------------------------\")\n",
        "print(spam_data.head())\n",
        "\n",
        "#Separate feature and target data\n",
        "spam_classes_raw = spam_data[\"CLASS\"]\n",
        "spam_messages = spam_data[\"SMS\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "64202dcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64202dcd",
        "outputId": "6c9b240d-3dd1-4974-8b0a-fc5141eb1b59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix Shape :  (1500, 4566)\n",
            "One-hot Encoding Shape :  (1500, 2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import tensorflow as tf\n",
        "\n",
        "#Custom tokenizer to remove stopwords and use lemmatization\n",
        "def customtokenize(str):\n",
        "    #Split string as tokens\n",
        "    tokens=nltk.word_tokenize(str)\n",
        "\n",
        "    nostop = list(filter(lambda token: token not in stopwords.words('english'), tokens))\n",
        "    #Perform lemmatization\n",
        "    lemmatized=[lemmatizer.lemmatize(word) for word in nostop ]\n",
        "    return lemmatized\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Build a TF-IDF Vectorizer model\n",
        "vectorizer = TfidfVectorizer(tokenizer=customtokenize)\n",
        "\n",
        "#Transform feature input to TF-IDF\n",
        "tfidf=vectorizer.fit_transform(spam_messages)\n",
        "#Convert TF-IDF to numpy array\n",
        "tfidf_array = tfidf.toarray()\n",
        "\n",
        "#Build a label encoder for target variable to convert strings to numeric values.\n",
        "from sklearn import preprocessing\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "spam_classes = label_encoder.fit_transform(\n",
        "                                spam_classes_raw)\n",
        "\n",
        "#Convert target to one-hot encoding vector\n",
        "spam_classes = tf.keras.utils.to_categorical(spam_classes,2)\n",
        "\n",
        "print(\"TF-IDF Matrix Shape : \", tfidf.shape)\n",
        "print(\"One-hot Encoding Shape : \", spam_classes.shape)\n",
        "\n",
        "X_train,X_test,Y_train,Y_test = train_test_split( tfidf_array, spam_classes, test_size=0.10)"
      ]
    },
    {
      "source": [
        "# Import the necessary modules\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Download the punkt tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define the custom tokenizer\n",
        "def customtokenize(str):\n",
        "    # Split the string into tokens\n",
        "    tokens = nltk.word_tokenize(str)\n",
        "\n",
        "\n",
        "\n",
        "    # Perform lemmatization\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in nostop]\n",
        "\n",
        "    return lemmatized\n",
        "\n",
        "# Create a TF-IDF Vectorizer model\n",
        "vectorizer = TfidfVectorizer(tokenizer=customtokenize)\n",
        "\n",
        "# Transform the feature input to TF-IDF\n",
        "tfidf = vectorizer.fit_transform(spam_messages)\n",
        "\n",
        "# Convert TF-IDF to numpy array\n",
        "tfidf_array = tfidf.toarray()\n",
        "\n",
        "# Build a label encoder for the target variable\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "spam_classes = label_encoder.fit_transform(spam_classes_raw)\n",
        "\n",
        "# Convert the target to one-hot encoding vector\n",
        "spam_classes = tf.keras.utils.to_categorical(spam_classes, 2)\n",
        "\n",
        "# Print the shape of the TF-IDF matrix and the one-hot encoding vector\n",
        "print(\"TF-IDF Matrix Shape : \", tfidf.shape)\n",
        "print(\"One-hot Encoding Shape : \", spam_classes.shape)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(tfidf_array, spam_classes, test_size=0.10)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "F4HlRhmkaIt3"
      },
      "id": "F4HlRhmkaIt3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "585a983f",
      "metadata": {
        "id": "585a983f"
      },
      "source": [
        "### 5.3. Building and Evaluating the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "294ceb7c",
      "metadata": {
        "id": "294ceb7c",
        "outputId": "2adef42d-9d23-4446-85f6-f72845f97fa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ab880e14ceee>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mVALIDATION_SPLIT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining Progress:\\n------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Set VERBOSE flag to 1 for displaying training progress\n",
        "VERBOSE = 1\n",
        "\n",
        "# Setup hyperparameters for training\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 10\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# Compile the model with specified optimizer, loss, and metrics\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display a message indicating the start of the training progress\n",
        "print(\"\\nTraining Progress:\\n------------------------------------\")\n",
        "\n",
        "# Train the model on the training data, specifying batch size, epochs, verbosity, and validation split\n",
        "history = model.fit(X_train,\n",
        "                    Y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    verbose=VERBOSE,\n",
        "                    validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# Display a message indicating the end of the training progress\n",
        "print(\"\\nAccuracy during Training :\\n------------------------------------\")\n",
        "\n",
        "# Import necessary libraries for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Plot the training accuracy over epochs\n",
        "pd.DataFrame(history.history)[\"accuracy\"].plot(figsize=(8, 5))\n",
        "plt.title(\"Accuracy improvements with Epoch\")\n",
        "plt.show()\n",
        "\n",
        "# Display a message indicating the start of the evaluation on the test dataset\n",
        "print(\"\\nEvaluation against Test Dataset :\\n------------------------------------\")\n",
        "\n",
        "# Evaluate the model on the test dataset and print the results\n",
        "model.evaluate(X_test, Y_test)\n"
      ]
    },
    {
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "cFjV02LSd7b3",
        "outputId": "b5f45552-b93e-4eee-d678-95a4f9fd9c1a"
      },
      "id": "cFjV02LSd7b3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-11ed3973292c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'compile'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9d4714",
      "metadata": {
        "id": "1d9d4714"
      },
      "source": [
        "### 5.4. Predicting for Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb0bdcc1",
      "metadata": {
        "id": "eb0bdcc1",
        "outputId": "ff20577f-c9d0-42af-ae88-3e1842fd6018",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-94cf801d1b98>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Convert input into IF-IDF vector using the same vectorizer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m predict_tfidf=vectorizer.transform([\"FREE entry to a fun contest\",\n\u001b[0m\u001b[1;32m      5\u001b[0m                                     \"Yup I will come over\"]).toarray()\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2155\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The TF-IDF vectorizer is not fitted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             )\n\u001b[0;32m-> 1430\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
          ]
        }
      ],
      "source": [
        "#Predict for multiple samples using batch processing\n",
        "\n",
        "#Convert input into IF-IDF vector using the same vectorizer model\n",
        "predict_tfidf=vectorizer.transform([\"FREE entry to a fun contest\",\n",
        "                                    \"Yup I will come over\"]).toarray()\n",
        "\n",
        "print(predict_tfidf.shape)\n",
        "\n",
        "#Predict using model\n",
        "prediction=np.argmax( model.predict(predict_tfidf), axis=1 )\n",
        "print(\"Prediction Output:\" , prediction)\n",
        "\n",
        "#Print prediction classes\n",
        "print(\"Prediction Classes are \", label_encoder.inverse_transform(prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6e1e04",
      "metadata": {
        "id": "2a6e1e04"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}